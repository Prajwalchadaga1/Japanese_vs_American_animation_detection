{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import print_function\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom keras.utils import np_utils \nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import optimizers\nfrom keras import losses\nfrom keras.models import load_model\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Load the training dataset ~87K states\nall_train = pd.read_csv(\"../input/applied-ai-assignment-2/Assignment_2_train.csv\")\nall_train.loc[(all_train.state == 4),'state']=0\nall_train.loc[(all_train.state == 5),'state']=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(all_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_train[1:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de146ebf369e33ab93a6769c019e351f7b6e9019","trusted":true},"cell_type":"code","source":"#Create a train/validation split\ndata_to_use = .2\ntrain=all_train[:int(len(all_train)*data_to_use)]\nsplit = .9\n\nTrain = train[:int(len(train)*split)]\nValid = train[int(len(train)*split):]\n\n\n#Remove the first and last column from the data, as it is the board name and the label\nX_train = Train.iloc[:, 1:-1].values\nX_valid = Valid.iloc[:, 1:-1].values\n\n#Remove everything except the last column from the data, as it is the label and put it in y\ny_train = Train.iloc[:, -1:].values\ny_valid = Valid.iloc[:, -1:].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(y_valid)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4a96ff453850ba077bfb90eca704c7c4a2e4eff","trusted":true},"cell_type":"code","source":"sample_train = X_train[50].reshape(-1,6,7)[0]\nsample_train","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c40d471b5baae0c9e2005154e85d89805980046d","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n#plot the first image in the dataset\nplt.imshow(sample_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set input to the shape of one X value\ndimof_input = X_train.shape[1]\n\n# Set y categorical\ndimof_output = int(np.max(y_train)+1)\ny_train = np_utils.to_categorical(y_train, dimof_output)\ny_valid = np_utils.to_categorical(y_valid, dimof_output)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test the time\n\nimport time\nfrom keras import initializers\nglorot = initializers.glorot_normal(seed=None)\n\nmlp_model = Sequential()\n\nmlp_model.add(Dense(100, input_dim=dimof_input, kernel_initializer=glorot, activation='relu'))\n\nmlp_model.add(Dense(dimof_output, kernel_initializer=glorot, activation='softmax')) #do not change\n\n#suggested actvations\n# 'relu'\n# 'sigmoid'\n# 'tanh'\n\n\n#suggested optimizers \nsgd = optimizers.SGD(learning_rate=0.01) #default lr = 0.01\nadagrad = optimizers.Adagrad(learning_rate=0.01) #default lr = 0.01\nadadelta = optimizers.Adadelta(learning_rate=1.0) #default lr = 1.0\nadam = optimizers.Adam(learning_rate=0.001) #default lr = 0.001\nadamax = optimizers.Adamax(learning_rate=0.002) #default lr = 0.002\nnadam = optimizers.Nadam(learning_rate=0.002) #default lr = 0.002\n\n\n# Suggested loss functions\ncat_cross = losses.categorical_crossentropy\nmse = losses.mean_squared_error\nbinary = losses.binary_crossentropy\n\n\nmlp_model.compile(loss=mse,   # **** pick any suggested loss functions\n                  optimizer=adam, # **** pick any suggested optimizers\n                  metrics=['accuracy']) #do not change\n\nstart = time.time()\n\nhistory = mlp_model.fit(x=X_train, \n                        y=y_train, \n                        batch_size=10000, # **** set from 1 to length of training data\n                        epochs=50, #pick any number higher will take longer\n                        verbose=1, # allows you to see more info per epoch\n                        validation_data=(X_valid,y_valid), \n                        validation_freq=1,\n                        shuffle=True)\n\ntotal_time = time.time()-start\nprint('Average time per epoch = '+str(total_time/5)+' seconds')\nprint('Aproximate time for 1000 epochs = '+str((total_time/5*1000)/60)+' minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nmlp_model = Sequential()\n\nmlp_model.add(Dense(100, input_dim=dimof_input, kernel_initializer=glorot, activation='relu'))\n\n\nmlp_model.add(Dense(dimof_output, kernel_initializer=glorot, activation='softmax')) #do not change\n#suggested actvations\n# 'relu'\n# 'sigmoid'\n# 'tanh'\n\n\n#suggested optimizers \nsgd = optimizers.SGD(learning_rate=0.01) #default lr = 0.01\nadagrad = optimizers.Adagrad(learning_rate=0.01) #default lr = 0.01\nadadelta = optimizers.Adadelta(learning_rate=1.0) #default lr = 1.0\nadam = optimizers.Adam(learning_rate=0.001) #default lr = 0.001\nadamax = optimizers.Adamax(learning_rate=0.002) #default lr = 0.002\nnadam = optimizers.Nadam(learning_rate=0.002) #default lr = 0.002\n\n\n# Suggested loss functions\ncat_cross = losses.categorical_crossentropy\nmse = losses.mean_squared_error\nbinary = losses.binary_crossentropy\n\n\nmlp_model.compile(loss=mse,   # **** pick any suggested loss functions\n                  optimizer=adam, # **** pick any suggested optimizers\n                  metrics=['accuracy']) #do not change\n\nstart = time.time()\n\nhistory = mlp_model.fit(x=X_train, \n                        y=y_train, \n                        batch_size=10000, # **** set from 1 to length of training data\n                        epochs=50, #pick any number higher will take longer\n                        verbose=0, # allows you to see more info per epoch\n                        validation_data=(X_valid,y_valid), \n                        validation_freq=1,\n                        shuffle=True)\n\n# evaluate the model\n_, train_acc = mlp_model.evaluate(X_train, y_train, verbose=0)\n_, valid_acc = mlp_model.evaluate(X_valid, y_valid, verbose=0)\nprint('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))\nprint(time.time()-start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model using early stopping and checkpoints\nimport time\nfrom keras import initializers\nglorot = initializers.glorot_normal(seed=None)\n\nmlp_model = Sequential()\n\n#create your network here\n\nmlp_model.add(Dense(32, input_dim=dimof_input, kernel_initializer=glorot, activation='sigmoid'))\n#mlp_model.add(BatchNormalization())\nmlp_model.add(Dropout(0.5))\n#mlp_model.add(BatchNormalization())\n\n#mlp_model.add(Dense(64,kernel_initializer='uniform',kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n\n\nmlp_model.add(Dense(dimof_output, kernel_initializer=glorot, activation='softmax')) #do not change\n\n\n#suggested optimizers \nsgd = optimizers.SGD(learning_rate=0.01) #default lr = 0.01\nadagrad = optimizers.Adagrad(learning_rate=0.01) #default lr = 0.01\nadadelta = optimizers.Adadelta(learning_rate=1.0) #default lr = 1.0\nadam = optimizers.Adam(learning_rate=0.001) #default lr = 0.001\nadamax = optimizers.Adamax(learning_rate=0.002) #default lr = 0.002\nnadam = optimizers.Nadam(learning_rate=0.002) #default lr = 0.002\n\n\n# Suggested loss functions\ncat_cross = losses.categorical_crossentropy\nmse = losses.mean_squared_error\nbinary = losses.binary_crossentropy\n\n\nmlp_model.compile(loss=mse,   # **** pick any suggested loss functions\n                  optimizer=adam, # **** pick any suggested optimizers\n                  metrics=['accuracy']) #do not change\n\nstart = time.time()\n\nes = EarlyStopping(monitor='val_loss', #do not change\n                   mode='min',  #do not change\n                   verbose=1, # allows you to see more info per epoch\n                   patience=50) # **** patience is how many validations to wait with nothing learned (patience * validation_freq)\n\nmc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True) #do not change\n\nhistory = mlp_model.fit(x=X_train, \n                        y=y_train, \n                        batch_size=7900, # **** set from 1 to length of training data\n                        epochs=10000, #do not change\n                        verbose=0, # allows you to see more info per epoch\n                        callbacks=[es, mc],\n                        validation_data=(X_valid,y_valid), \n                        validation_freq=1,\n                        shuffle=True)\n\n#load the best model\nsaved_model = load_model('best_model.h5')\n\n# evaluate the model\n_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n_, valid_acc = saved_model.evaluate(X_valid, y_valid, verbose=0)\nprint('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))\nprint(time.time()-start)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example grid search\n\nimport time\nnode = 2048*2\nwhile node > 2:\n    mlp_model = Sequential()\n\n    mlp_model.add(Dense(node, input_dim=dimof_input, kernel_initializer='uniform', activation='relu'))\n    mlp_model.add(Dropout(0.2))\n    #mlp_model.add(BatchNormalization())\n    #mlp_model.add(Dense(512,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.2))\n    #mlp_model.add(Dense(64,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.5))\n    #mlp_model.add(BatchNormalization())\n    #mlp_model.add(Dense(32,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.3))\n    #mlp_model.add(Dense(16,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.3))\n\n    mlp_model.add(Dense(dimof_output, kernel_initializer='uniform', activation='softmax')) #do not change\n\n\n    #suggested optimizers \n    sgd = optimizers.SGD(learning_rate=0.01) #default lr = 0.01\n    adagrad = optimizers.Adagrad(learning_rate=0.01) #default lr = 0.01\n    adadelta = optimizers.Adadelta(learning_rate=1.0) #default lr = 1.0\n    adam = optimizers.Adam(learning_rate=0.001) #default lr = 0.001\n    adamax = optimizers.Adamax(learning_rate=0.002) #default lr = 0.002\n    nadam = optimizers.Nadam(learning_rate=0.002) #default lr = 0.002\n\n\n    # Suggested loss functions\n    cat_cross = losses.categorical_crossentropy\n    mse = losses.mean_squared_error\n    binary = losses.binary_crossentropy\n\n\n    mlp_model.compile(loss=mse,   # **** pick any suggested loss functions\n                      optimizer=adam, # **** pick any suggested optimizers\n                      metrics=['accuracy']) #do not change\n\n    start = time.time()\n\n    es = EarlyStopping(monitor='val_loss', #do not change\n                       mode='min',  #do not change\n                       verbose=1, # allows you to see more info per epoch\n                       patience=50) # **** patience is how many validations to wait with nothing learned (patience * validation_freq)\n\n    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True) #do not change\n\n    history = mlp_model.fit(x=X_train, \n                            y=y_train, \n                            batch_size=7900, # **** set from 1 to length of training data\n                            epochs=10000, #do not change\n                            verbose=0, # allows you to see more info per epoch\n                            callbacks=[es, mc],\n                            validation_data=(X_valid,y_valid), \n                            validation_freq=1,\n                            shuffle=True)\n\n    #load the best model\n    saved_model = load_model('best_model.h5')\n\n    # evaluate the model\n    _, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n    _, valid_acc = saved_model.evaluate(X_valid, y_valid, verbose=0)\n    print('Node - '+str(node))\n    print('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))\n    print(time.time()-start)\n    node = int(node/2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test for standard deviation of a model\n\nimport time\nvalid_list = []\nfor i in range(10):\n    mlp_model = Sequential()\n\n    mlp_model.add(Dense(64, input_dim=dimof_input, kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.5))\n    #mlp_model.add(BatchNormalization())\n    #mlp_model.add(Dense(512,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.2))\n    #mlp_model.add(Dense(64,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.5))\n    #mlp_model.add(BatchNormalization())\n    #mlp_model.add(Dense(32,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.3))\n    #mlp_model.add(Dense(16,kernel_initializer='uniform', activation='relu'))\n    #mlp_model.add(Dropout(0.3))\n\n    mlp_model.add(Dense(dimof_output, kernel_initializer='uniform', activation='softmax')) #do not change\n\n\n    #suggested optimizers \n    sgd = optimizers.SGD(learning_rate=0.01) #default lr = 0.01\n    adagrad = optimizers.Adagrad(learning_rate=0.01) #default lr = 0.01\n    adadelta = optimizers.Adadelta(learning_rate=1.0) #default lr = 1.0\n    adam = optimizers.Adam(learning_rate=0.001) #default lr = 0.001\n    adamax = optimizers.Adamax(learning_rate=0.002) #default lr = 0.002\n    nadam = optimizers.Nadam(learning_rate=0.002) #default lr = 0.002\n\n\n    # Suggested loss functions\n    cat_cross = losses.categorical_crossentropy\n    mse = losses.mean_squared_error\n    binary = losses.binary_crossentropy\n\n\n    mlp_model.compile(loss=mse,   # **** pick any suggested loss functions\n                      optimizer=adam, # **** pick any suggested optimizers\n                      metrics=['accuracy']) #do not change\n\n    start = time.time()\n\n    es = EarlyStopping(monitor='val_loss', #do not change\n                       mode='min',  #do not change\n                       verbose=1, # allows you to see more info per epoch\n                       patience=50) # **** patience is how many validations to wait with nothing learned (patience * validation_freq)\n\n    mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True) #do not change\n\n    history = mlp_model.fit(x=X_train, \n                            y=y_train, \n                            batch_size=7900, # **** set from 1 to length of training data\n                            epochs=10000, #do not change\n                            verbose=0, # allows you to see more info per epoch\n                            callbacks=[es, mc],\n                            validation_data=(X_valid,y_valid), \n                            validation_freq=1,\n                            shuffle=True)\n\n    #load the best model\n    saved_model = load_model('best_model.h5')\n\n    # evaluate the model\n    _, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n    _, valid_acc = saved_model.evaluate(X_valid, y_valid, verbose=0)\n    valid_list.append(valid_acc)\n    print('Train: %.3f, Valid: %.3f' % (train_acc, valid_acc))\n    print(time.time()-start)\nprint('Average Score- '+ str(np.mean(valid_list)))\nprint('Standard deviation - '+ str(np.std(valid_list)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Go here when your saved model is ready\ntest = pd.read_csv(\"../input/applied-ai-assignment-2/Assignment_2_test.csv\")\ntest.loc[(test.state == 4),'state']=0\ntest.loc[(test.state == 5),'state']=1\nX_test = test.iloc[:, 1:-1].values\ny_test = test.iloc[:, -1:].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creates the final output \nlist_of_boards = [i for i in list(test['file_names'])]\nresult = saved_model.predict(X_test)\ntest_results = []\nfor i in result:\n    test_results.append(np.argmax(i))\n#Creates a dataframe that can be saved as a csv for submission\nsubmission_data = pd.DataFrame(\n    {'BoardId': list_of_boards,\n     'Label': test_results\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_data[1:9]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"263d012fb4f24b54c6c4b2d269b795963475df65","trusted":true},"cell_type":"code","source":"submission_data.to_csv('submission.csv', sep=',',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python (tensor_p27)","language":"python","name":"tensorflow_p27"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15"}},"nbformat":4,"nbformat_minor":1}